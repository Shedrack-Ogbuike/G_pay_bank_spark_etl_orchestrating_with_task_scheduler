{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49da7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing pyspark on a local machine\n",
    "# import Necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import DataFrameWriter\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import os\n",
    "import psycopg2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870aa9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BanksETL\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\", r\"C:\\Users\\user\\Desktop\\G_pay_bank\\postgresql-42.7.7.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd8782a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-NV8LV4E.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BanksETL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1602c68e660>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6206fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract  this history data into a spark dataframe\n",
    "df = spark.read.csv(r\"dataset\\rawdata\\G_pay_bank_transactions.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb7113a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------------+--------------+--------------------+------------------+--------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+------+--------------+\n",
      "|    Transaction_Date|Amount|Transaction_Type| Customer_Name|    Customer_Address|     Customer_City|Customer_State|    Customer_Country|             Company|           Job_Title|               Email|       Phone_Number|Credit_Card_Number|                IBAN|Currency_Code|Random_Number|Category|Group|Is_Active|        Last_Updated|         Description|Gender|Marital_Status|\n",
      "+--------------------+------+----------------+--------------+--------------------+------------------+--------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+------+--------------+\n",
      "|2024-03-23 15:38:...| 34.76|      Withdrawal|    James Neal|54912 Holmes Lodg...| West Keithborough|       Florida|                Togo|Benson, Johnson a...|                NULL|                NULL|  493.720.6609x7545|  3592901394693441|GB98RBPP090285271...|          MAD|       3167.0|       C|    Z|       No|2020-06-20 03:04:...|Yeah food anythin...| Other|      Divorced|\n",
      "|2024-04-22 19:15:...|163.92|      Withdrawal|   Thomas Long| 1133 Collin Passage|        Joshuabury|   Connecticut|Lao People's Demo...|                NULL|   Food technologist|michellelynch@exa...|      (497)554-3317|              NULL|GB03KFZR339662263...|          VEF|       2122.0|       B|    Z|     NULL|2020-12-27 13:23:...|Teach edge make n...|Female|       Married|\n",
      "|2024-04-12 19:46:...|386.32|      Withdrawal|Ashley Shelton|5297 Johnson Port...|       North Maria|    New Jersey|              Bhutan|       Jones-Mueller|Database administ...| ljordan@example.org|      (534)769-3072|      675983949974|GB59QYRN446730519...|          COP|       7796.0|       C|    Z|       No|2020-01-24 01:23:...|Again line face c...| Other|          NULL|\n",
      "|2024-04-17 15:29:...|407.15|         Deposit| James Rosario|56955 Moore Glens...|North Michellefurt|    New Mexico|             Iceland|       Vargas-Harris|Horticultural the...|parkerjames@examp...|+1-447-900-1320x257|     4761202519057|GB74FTDO268299438...|          BWP|       6284.0|       C|    Z|      Yes|2023-09-27 03:01:...|     Bag my a drive.|  NULL|          NULL|\n",
      "|2024-02-10 01:51:...|161.31|         Deposit|Miguel Leonard|262 Beck Expressw...|              NULL| West Virginia|             Eritrea|Richardson, Gonza...|   Minerals surveyor| zweaver@example.net|               NULL|   213156729655186|GB94EWRN587847592...|          SOS|       9179.0|       C|    Y|       No|2022-01-22 19:08:...|Husband find ok w...|Female|       Married|\n",
      "+--------------------+------+----------------+--------------+--------------------+------------------+--------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7260bbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_Date: timestamp (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Transaction_Type: string (nullable = true)\n",
      " |-- Customer_Name: string (nullable = true)\n",
      " |-- Customer_Address: string (nullable = true)\n",
      " |-- Customer_City: string (nullable = true)\n",
      " |-- Customer_State: string (nullable = true)\n",
      " |-- Customer_Country: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Job_Title: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Phone_Number: string (nullable = true)\n",
      " |-- Credit_Card_Number: long (nullable = true)\n",
      " |-- IBAN: string (nullable = true)\n",
      " |-- Currency_Code: string (nullable = true)\n",
      " |-- Random_Number: double (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Group: string (nullable = true)\n",
      " |-- Is_Active: string (nullable = true)\n",
      " |-- Last_Updated: timestamp (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c57d593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Transaction_Date, Nulls: 0\n",
      "Column: Amount, Nulls: 0\n",
      "Column: Transaction_Type, Nulls: 0\n",
      "Column: Customer_Name, Nulls: 100425\n",
      "Column: Customer_Address, Nulls: 100087\n",
      "Column: Customer_City, Nulls: 100034\n",
      "Column: Customer_State, Nulls: 100009\n",
      "Column: Customer_Country, Nulls: 100672\n",
      "Column: Company, Nulls: 100295\n",
      "Column: Job_Title, Nulls: 99924\n",
      "Column: Email, Nulls: 100043\n",
      "Column: Phone_Number, Nulls: 100524\n",
      "Column: Credit_Card_Number, Nulls: 100085\n",
      "Column: IBAN, Nulls: 100300\n",
      "Column: Currency_Code, Nulls: 99342\n",
      "Column: Random_Number, Nulls: 99913\n",
      "Column: Category, Nulls: 100332\n",
      "Column: Group, Nulls: 100209\n",
      "Column: Is_Active, Nulls: 100259\n",
      "Column: Last_Updated, Nulls: 100321\n",
      "Column: Description, Nulls: 100403\n",
      "Column: Gender, Nulls: 99767\n",
      "Column: Marital_Status, Nulls: 99904\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and transformation\n",
    "for col in df.columns:\n",
    "    print(f\"Column: {col}, Nulls: {df.filter(df[col].isNull()).count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c82fed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------------+-------------+--------------------+-------------+--------------+----------------+-------------+------------------+-------------------+-------------------+--------------------+--------------------+-------------+------------------+--------+------+---------+--------------------+------+--------------+\n",
      "|summary|           Amount|Transaction_Type|Customer_Name|    Customer_Address|Customer_City|Customer_State|Customer_Country|      Company|         Job_Title|              Email|       Phone_Number|  Credit_Card_Number|                IBAN|Currency_Code|     Random_Number|Category| Group|Is_Active|         Description|Gender|Marital_Status|\n",
      "+-------+-----------------+----------------+-------------+--------------------+-------------+--------------+----------------+-------------+------------------+-------------------+-------------------+--------------------+--------------------+-------------+------------------+--------+------+---------+--------------------+------+--------------+\n",
      "|  count|          1000000|         1000000|       899575|              899913|       899966|        899991|          899328|       899705|            900076|             899957|             899476|              899915|              899700|       900658|            900087|  899668|899791|   899741|              899597|900233|        900096|\n",
      "|   mean|504.9737112199981|            NULL|         NULL|                NULL|         NULL|          NULL|            NULL|         NULL|              NULL|               NULL| 6.00341888622502E9|3.785330130644790...|                NULL|         NULL| 5504.445295843624|    NULL|  NULL|     NULL|                NULL|  NULL|          NULL|\n",
      "| stddev|285.7997202441234|            NULL|         NULL|                NULL|         NULL|          NULL|            NULL|         NULL|              NULL|               NULL|2.307111981222392E9|1.247765855631418...|                NULL|         NULL|2598.1680843741237|    NULL|  NULL|     NULL|                NULL|  NULL|          NULL|\n",
      "|    min|             10.0|         Deposit| Aaron Abbott|000 Aaron Landing...|    Aaronberg|       Alabama|     Afghanistan| Abbott Group|Academic librarian|aabbott@example.com|      (200)201-4254|         60400015693|GB02AAAU191993009...|          AED|            1000.0|       A|     X|       No|A American and to...|Female|      Divorced|\n",
      "|    max|           1000.0|      Withdrawal|    Zoe Young|    99999 Mark Vista|  Zunigaville|       Wyoming|        Zimbabwe|Zuniga-Wilson|      Youth worker|zzuniga@example.org|         9999777682| 4999984361512569455|GB98ZZXM257326775...|          ZWD|            9999.0|       D|     Z|      Yes|Yourself young ev...| Other|        Single|\n",
      "+-------+-----------------+----------------+-------------+--------------------+-------------+--------------+----------------+-------------+------------------+-------------------+-------------------+--------------------+--------------------+-------------+------------------+--------+------+---------+--------------------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6975c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill up the missing values\n",
    "df_cleaned = df.fillna({\n",
    "    \"Customer_Name\": \"Unknown\",\n",
    "    \"Customer_Address\": \"Unknown\",\n",
    "    \"Customer_City\": \"Unknown\",\n",
    "    \"Customer_State\": \"Unknown\",\n",
    "    \"Customer_Country\": \"Unknown\",\n",
    "    \"Company\": \"Unknown\",\n",
    "    \"Job_Title\": \"Unknown\",\n",
    "    \"Email\": \"Unknown\",\n",
    "    \"Phone_Number\": \"Unknown\",\n",
    "    \"Credit_Card_Number\": 0,\n",
    "    \"IBAN\": \"Unknown\",\n",
    "    \"Currency_Code\": \"Unknown\",\n",
    "    \"Random_Number\": 0.0,\n",
    "    \"Category\": \"Unknown\",\n",
    "    \"Group\": \"Unknown\",\n",
    "    \"Is_Active\": \"Unknown\",\n",
    "    \"Description\": \"No Description\",\n",
    "    \"Gender\": \"Unknown\",\n",
    "    \"Marital_Status\": \"Unknown\"\n",
    "})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfe934a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the missing values in the Last_Updated column\n",
    "df_cleaned = df_cleaned.dropna(subset=[\"Last_Updated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45b8f93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: Transaction_Date, 'Nulls: ', 0\n",
      "Column: Amount, 'Nulls: ', 0\n",
      "Column: Transaction_Type, 'Nulls: ', 0\n",
      "Column: Customer_Name, 'Nulls: ', 0\n",
      "Column: Customer_Address, 'Nulls: ', 0\n",
      "Column: Customer_City, 'Nulls: ', 0\n",
      "Column: Customer_State, 'Nulls: ', 0\n",
      "Column: Customer_Country, 'Nulls: ', 0\n",
      "Column: Company, 'Nulls: ', 0\n",
      "Column: Job_Title, 'Nulls: ', 0\n",
      "Column: Email, 'Nulls: ', 0\n",
      "Column: Phone_Number, 'Nulls: ', 0\n",
      "Column: Credit_Card_Number, 'Nulls: ', 0\n",
      "Column: IBAN, 'Nulls: ', 0\n",
      "Column: Currency_Code, 'Nulls: ', 0\n",
      "Column: Random_Number, 'Nulls: ', 0\n",
      "Column: Category, 'Nulls: ', 0\n",
      "Column: Group, 'Nulls: ', 0\n",
      "Column: Is_Active, 'Nulls: ', 0\n",
      "Column: Last_Updated, 'Nulls: ', 0\n",
      "Column: Description, 'Nulls: ', 0\n",
      "Column: Gender, 'Nulls: ', 0\n",
      "Column: Marital_Status, 'Nulls: ', 0\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and transformation\n",
    "for col in df_cleaned.columns:\n",
    "    print(f\"Column: {col}, 'Nulls: ', {df_cleaned.filter(df_cleaned[col].isNull()).count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54cf20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation\n",
    "# Transaction table\n",
    "transactions = df_cleaned.select(\"Transaction_Date\",\"Amount\",\"Transaction_Type\")\\\n",
    "                         .withColumn(\"Transaction_ID\", monotonically_increasing_id())\\\n",
    "                         .select(\"Transaction_ID\", \"Transaction_Date\", \"Amount\", \"Transaction_Type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e2b9b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+------+----------------+\n",
      "|Transaction_ID|    Transaction_Date|Amount|Transaction_Type|\n",
      "+--------------+--------------------+------+----------------+\n",
      "|             0|2024-03-23 15:38:...| 34.76|      Withdrawal|\n",
      "|             1|2024-04-22 19:15:...|163.92|      Withdrawal|\n",
      "|             2|2024-04-12 19:46:...|386.32|      Withdrawal|\n",
      "|             3|2024-04-17 15:29:...|407.15|         Deposit|\n",
      "|             4|2024-02-10 01:51:...|161.31|         Deposit|\n",
      "+--------------+--------------------+------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1a01334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer table\n",
    "customers = df_cleaned.select( \"Customer_Name\", \"Customer_Address\", \"Customer_City\", \n",
    "                               \"Customer_State\", \"Customer_Country\")\\\n",
    "                     .withColumn(\"Customer_ID\", monotonically_increasing_id())\\\n",
    "                        .select(\"Customer_ID\", \"Customer_Name\", \"Customer_Address\", \n",
    "                                \"Customer_City\", \"Customer_State\", \"Customer_Country\")\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e94f8f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+------------------+--------------+--------------------+\n",
      "|Customer_ID| Customer_Name|    Customer_Address|     Customer_City|Customer_State|    Customer_Country|\n",
      "+-----------+--------------+--------------------+------------------+--------------+--------------------+\n",
      "|          0|    James Neal|54912 Holmes Lodg...| West Keithborough|       Florida|                Togo|\n",
      "|          1|   Thomas Long| 1133 Collin Passage|        Joshuabury|   Connecticut|Lao People's Demo...|\n",
      "|          2|Ashley Shelton|5297 Johnson Port...|       North Maria|    New Jersey|              Bhutan|\n",
      "|          3| James Rosario|56955 Moore Glens...|North Michellefurt|    New Mexico|             Iceland|\n",
      "|          4|Miguel Leonard|262 Beck Expressw...|           Unknown| West Virginia|             Eritrea|\n",
      "+-----------+--------------+--------------------+------------------+--------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "customers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96d52692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# employee table\n",
    "employees = df_cleaned.select(\"company\", \"job_title\", \"email\", \"phone_number\", \"Gender\", \"Marital_Status\")\\\n",
    "                        .withColumn(\"Employee_ID\", monotonically_increasing_id())\\\n",
    "                        .select(\"Employee_ID\", \"company\", \"job_title\", \"email\", \"phone_number\",  \"Gender\", \"Marital_Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0279ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+-------------------+-------+--------------+\n",
      "|Employee_ID|             company|           job_title|               email|       phone_number| Gender|Marital_Status|\n",
      "+-----------+--------------------+--------------------+--------------------+-------------------+-------+--------------+\n",
      "|          0|Benson, Johnson a...|             Unknown|             Unknown|  493.720.6609x7545|  Other|      Divorced|\n",
      "|          1|             Unknown|   Food technologist|michellelynch@exa...|      (497)554-3317| Female|       Married|\n",
      "|          2|       Jones-Mueller|Database administ...| ljordan@example.org|      (534)769-3072|  Other|       Unknown|\n",
      "|          3|       Vargas-Harris|Horticultural the...|parkerjames@examp...|+1-447-900-1320x257|Unknown|       Unknown|\n",
      "|          4|Richardson, Gonza...|   Minerals surveyor| zweaver@example.net|            Unknown| Female|       Married|\n",
      "+-----------+--------------------+--------------------+--------------------+-------------------+-------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "employees.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04d0e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact table\n",
    "fact_table = df_cleaned.join(transactions, [\"Transaction_Date\",\"Amount\",\"Transaction_Type\"], how=\"inner\")\\\n",
    "    .join(customers, [ \"Customer_Name\", \"Customer_Address\", \"Customer_City\", \n",
    "                               \"Customer_State\", \"Customer_Country\"], how=\"inner\")\\\n",
    "    .join(employees, [\"company\", \"job_title\", \"email\", \"phone_number\", \"Gender\", \"Marital_Status\"] , how=\"inner\")\\\n",
    "    .select('Transaction_ID', 'Customer_ID', 'Employee_ID','Transaction_Date',  'Credit_Card_Number',\\\n",
    "     'IBAN','Currency_Code','Random_Number','Category','Group','Is_Active','Last_Updated','Description')\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "997d8542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-----------+--------------------+-------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+\n",
      "|Transaction_ID|Customer_ID|Employee_ID|    Transaction_Date| Credit_Card_Number|                IBAN|Currency_Code|Random_Number|Category|Group|Is_Active|        Last_Updated|         Description|\n",
      "+--------------+-----------+-----------+--------------------+-------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+\n",
      "|   17179921196|17179921196|17179921196|2024-01-03 05:07:...|4131053293826613966|GB78CKOS832138470...|          GNF|       7475.0|       B|    Z|      Yes|2022-02-06 23:35:...|Health sure story...|\n",
      "|         29992|      29992|      29992|2024-02-23 16:43:...|   2452924189738024|GB50ZGVC717046003...|          TRY|       9153.0|       D|    Z|       No|2021-11-26 08:47:...|Wait represent ge...|\n",
      "|   25769840960|25769840960|25769840960|2024-02-21 20:58:...|   4625846628264239|GB97NDWP370396356...|          BSD|       6117.0| Unknown|    X|      Yes|2023-02-14 08:41:...|Onto give past no...|\n",
      "|   60129626495|60129626495|60129626495|2024-04-26 22:02:...|                  0|GB11VVRM027865406...|      Unknown|       6669.0|       C|    X|       No|2022-04-13 17:22:...|Teach military ev...|\n",
      "|        103516|     103516|     103516|2024-02-07 05:19:...|     30167114834907|GB09TXTK261344054...|          PHP|          0.0|       C|    Z|      Yes|2022-11-03 23:14:...|Sport internation...|\n",
      "+--------------+-----------+-----------+--------------------+-------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "fact_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bd85118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\user\\appdata\\roaming\\python\\python313\\site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e1bb0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Get the password\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "# Data loading\n",
    "def get_db_connection():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"g_pay_bank\",\n",
    "        user=\"postgres\",\n",
    "        password=db_password\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "# connect to the database\n",
    "conn = get_db_connection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae867dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_ID: long (nullable = false)\n",
      " |-- Transaction_Date: timestamp (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Transaction_Type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "153fe104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables in the database\n",
    "def create_table():\n",
    "    cursor = conn.cursor()\n",
    "    create_table_query = \"\"\"\n",
    "        DROP TABLE IF EXISTS fact_table;\n",
    "        DROP TABLE IF EXISTS transactions;\n",
    "        DROP TABLE IF EXISTS customers;\n",
    "        DROP TABLE IF EXISTS employees;\n",
    "\n",
    "        CREATE TABLE transactions (\n",
    "            Transaction_ID BIGINT PRIMARY KEY,\n",
    "            Transaction_Date DATE NOT NULL,\n",
    "            Amount DECIMAL(18, 4) NOT NULL,\n",
    "            Transaction_Type VARCHAR(50) NOT NULL\n",
    "        );\n",
    "\n",
    "        CREATE TABLE customers (\n",
    "            Customer_ID BIGINT PRIMARY KEY,\n",
    "            Customer_Name VARCHAR(1000) NOT NULL,\n",
    "            Customer_Address VARCHAR(2550) NOT NULL,\n",
    "            Customer_City VARCHAR(1000) NOT NULL,\n",
    "            Customer_State VARCHAR(1000) NOT NULL,\n",
    "            Customer_Country VARCHAR(1000) NOT NULL\n",
    "        );\n",
    "\n",
    "        CREATE TABLE employees (\n",
    "            Employee_ID BIGINT PRIMARY KEY,\n",
    "            Company VARCHAR(1000) NOT NULL,\n",
    "            Job_Title VARCHAR(1000) NOT NULL,\n",
    "            Email VARCHAR(1000) NOT NULL,\n",
    "            Phone_Number VARCHAR(200) NOT NULL,\n",
    "            Gender VARCHAR(50) NOT NULL,\n",
    "            Marital_Status VARCHAR(200) NOT NULL\n",
    "        );\n",
    "\n",
    "        CREATE TABLE fact_table (\n",
    "            Transaction_ID BIGINT REFERENCES transactions(Transaction_ID),\n",
    "            Customer_ID BIGINT REFERENCES customers(Customer_ID),\n",
    "            Employee_ID BIGINT REFERENCES employees(Employee_ID),\n",
    "            Transaction_Date DATE NOT NULL,\n",
    "            Credit_Card_Number VARCHAR(200) NOT NULL,\n",
    "            IBAN VARCHAR(34) NOT NULL,\n",
    "            Currency_Code VARCHAR(100) NOT NULL,\n",
    "            Random_Number DOUBLE PRECISION NOT NULL,\n",
    "            Category VARCHAR(1000) NOT NULL,\n",
    "            \"Group\" VARCHAR(1000) NOT NULL,\n",
    "            Is_Active BOOLEAN NOT NULL,\n",
    "            Last_Updated TIMESTAMP NOT NULL,\n",
    "            Description TEXT\n",
    "        );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d501030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_ID: long (nullable = false)\n",
      " |-- Customer_ID: long (nullable = false)\n",
      " |-- Employee_ID: long (nullable = false)\n",
      " |-- Transaction_Date: timestamp (nullable = true)\n",
      " |-- Credit_Card_Number: long (nullable = false)\n",
      " |-- IBAN: string (nullable = false)\n",
      " |-- Currency_Code: string (nullable = false)\n",
      " |-- Random_Number: double (nullable = false)\n",
      " |-- Category: string (nullable = false)\n",
      " |-- Group: string (nullable = false)\n",
      " |-- Is_Active: string (nullable = false)\n",
      " |-- Last_Updated: timestamp (nullable = true)\n",
      " |-- Description: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfa8d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b621931",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"jdbc:postgresql://localhost:5432/g_pay_bank\"\n",
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": db_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Load data into the database\n",
    "#transactions.write.jdbc(url=url, table=\"transactions\", mode=\"append\", properties=properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45ba0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers.write.jdbc(url=url, table=\"customers\", mode=\"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db70e941",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o335.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 149.0 failed 1 times, most recent failure: Lost task 2.0 in stage 149.0 (TID 353) (DESKTOP-NV8LV4E.mshome.net executor driver): java.sql.BatchUpdateException: Batch entry 37 INSERT INTO employees (\"employee_id\",\"company\",\"job_title\",\"email\",\"phone_number\",\"gender\",\"marital_status\") VALUES (('17179869221'::int8),('Smith PLC'),('Illustrator'),('jenniferalvarado@example.org'),('+1-280-413-6383x28672'),('Male'),('Single')) was aborted: ERROR: value too long for type character varying(20)  Call getNextException to see other errors in the batch.\r\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\r\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2422)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2154)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1502)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1527)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:566)\r\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:886)\r\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:910)\r\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1778)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:837)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:995)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:994)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(20)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2734)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1047)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1045)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$foreachPartition$1(Dataset.scala:1474)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2221)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2219)\r\n\tat org.apache.spark.sql.classic.Dataset.foreachPartition(Dataset.scala:1474)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:994)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:71)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\t\tat scala.Option.foreach(Option.scala:437)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\r\n\t\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1047)\r\n\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\t\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\t\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1045)\r\n\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$foreachPartition$1(Dataset.scala:1474)\r\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2221)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2219)\r\n\t\tat org.apache.spark.sql.classic.Dataset.foreachPartition(Dataset.scala:1474)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:994)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:71)\r\n\t\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 20 more\r\nCaused by: java.sql.BatchUpdateException: Batch entry 37 INSERT INTO employees (\"employee_id\",\"company\",\"job_title\",\"email\",\"phone_number\",\"gender\",\"marital_status\") VALUES (('17179869221'::int8),('Smith PLC'),('Illustrator'),('jenniferalvarado@example.org'),('+1-280-413-6383x28672'),('Male'),('Single')) was aborted: ERROR: value too long for type character varying(20)  Call getNextException to see other errors in the batch.\r\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\r\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2422)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2154)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1502)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1527)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:566)\r\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:886)\r\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:910)\r\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1778)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:837)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:995)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:994)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(20)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2734)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\r\n\t... 24 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43memployees\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memployees\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m fact_table\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mjdbc(url\u001b[38;5;241m=\u001b[39murl, table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfact_table\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties\u001b[38;5;241m=\u001b[39mproperties)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\sql\\readwriter.py:2347\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[1;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[0;32m   2345\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[0;32m   2346\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[1;32m-> 2347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o335.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 149.0 failed 1 times, most recent failure: Lost task 2.0 in stage 149.0 (TID 353) (DESKTOP-NV8LV4E.mshome.net executor driver): java.sql.BatchUpdateException: Batch entry 37 INSERT INTO employees (\"employee_id\",\"company\",\"job_title\",\"email\",\"phone_number\",\"gender\",\"marital_status\") VALUES (('17179869221'::int8),('Smith PLC'),('Illustrator'),('jenniferalvarado@example.org'),('+1-280-413-6383x28672'),('Male'),('Single')) was aborted: ERROR: value too long for type character varying(20)  Call getNextException to see other errors in the batch.\r\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\r\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2422)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2154)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1502)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1527)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:566)\r\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:886)\r\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:910)\r\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1778)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:837)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:995)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:994)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(20)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2734)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1047)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1045)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$foreachPartition$1(Dataset.scala:1474)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2221)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2219)\r\n\tat org.apache.spark.sql.classic.Dataset.foreachPartition(Dataset.scala:1474)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:994)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:71)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:126)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:334)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\t\tat scala.Option.foreach(Option.scala:437)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\t\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\t\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\t\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\t\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\r\n\t\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1047)\r\n\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\t\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\t\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\t\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1045)\r\n\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$foreachPartition$1(Dataset.scala:1474)\r\n\t\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\t\tat org.apache.spark.sql.classic.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:2221)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.classic.Dataset.withNewRDDExecutionId(Dataset.scala:2219)\r\n\t\tat org.apache.spark.sql.classic.Dataset.foreachPartition(Dataset.scala:1474)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:994)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:71)\r\n\t\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:55)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:77)\r\n\t\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:88)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 20 more\r\nCaused by: java.sql.BatchUpdateException: Batch entry 37 INSERT INTO employees (\"employee_id\",\"company\",\"job_title\",\"email\",\"phone_number\",\"gender\",\"marital_status\") VALUES (('17179869221'::int8),('Smith PLC'),('Illustrator'),('jenniferalvarado@example.org'),('+1-280-413-6383x28672'),('Male'),('Single')) was aborted: ERROR: value too long for type character varying(20)  Call getNextException to see other errors in the batch.\r\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\r\n\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:52)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2422)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2154)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1502)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1527)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:566)\r\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:886)\r\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:910)\r\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1778)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:837)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:995)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:994)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1047)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1047)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(20)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2734)\r\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2421)\r\n\t... 24 more\r\n"
     ]
    }
   ],
   "source": [
    "employees.write.jdbc(url=url, table=\"employees\", mode=\"append\", properties=properties)\n",
    "fact_table.write.jdbc(url=url, table=\"fact_table\", mode=\"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ae261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
